@misc{nihNatureProtein,
	author = {},
	title = {Nature of the protein universe - {P}ub{M}ed --- pubmed.ncbi.nlm.nih.gov},
	howpublished = {\url{https://pubmed.ncbi.nlm.nih.gov/19541617/}},
	year = {},
	note = {[Accessed 23-11-2024]},
}

@misc{nihDarwinianEvolution,
	author = {},
	title = {{D}arwinian evolution can follow only very few mutational paths to fitter proteins - {P}ub{M}ed --- pubmed.ncbi.nlm.nih.gov},
	howpublished = {\url{https://pubmed.ncbi.nlm.nih.gov/16601193/}},
	year = {},
	note = {[Accessed 23-11-2024]},
}

@misc{natureHighlyAccurate,
	author = {},
	title = {{H}ighly accurate protein structure prediction with {A}lpha{F}old - {N}ature --- nature.com},
	howpublished = {\url{https://www.nature.com/articles/s41586-021-03819-2#citeas}},
	year = {},
	note = {[Accessed 23-11-2024]},
}

@misc{nihEvolutionbasedModel,
	author = {},
	title = {{A}n evolution-based model for designing chorismate mutase enzymes - {P}ub{M}ed --- pubmed.ncbi.nlm.nih.gov},
	howpublished = {\url{https://pubmed.ncbi.nlm.nih.gov/32703877/}},
	year = {},
	note = {[Accessed 23-11-2024]},
}

@misc{natureUnifiedRational,
	author = {},
	title = {{U}nified rational protein engineering with sequence-based deep representation learning - {N}ature {M}ethods --- nature.com},
	howpublished = {\url{https://www.nature.com/articles/s41592-019-0598-1#citeas}},
	year = {},
	note = {[Accessed 23-11-2024]},
}

@misc{riesselman2017deepgenerativemodelsgenetic,
      title={Deep generative models of genetic variation capture mutation effects}, 
      author={Adam J. Riesselman and John B. Ingraham and Debora S. Marks},
      year={2017},
      eprint={1712.06527},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/1712.06527}, 
}

@inproceedings{NEURIPS2020_4c5bcfec,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{rao2019evaluatingproteintransferlearning,
      title={Evaluating Protein Transfer Learning with TAPE}, 
      author={Roshan Rao and Nicholas Bhattacharya and Neil Thomas and Yan Duan and Xi Chen and John Canny and Pieter Abbeel and Yun S. Song},
      year={2019},
      eprint={1906.08230},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.08230}, 
}

@misc{elnaggar2021prottranscrackinglanguagelifes,
      title={ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing}, 
      author={Ahmed Elnaggar and Michael Heinzinger and Christian Dallago and Ghalia Rihawi and Yu Wang and Llion Jones and Tom Gibbs and Tamas Feher and Christoph Angerer and Martin Steinegger and Debsindhu Bhowmik and Burkhard Rost},
      year={2021},
      eprint={2007.06225},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.06225}, 
}

@article{louis,
author = {Iain G. Johnston  and Kamaludin Dingle  and Sam F. Greenbury  and Chico Q. Camargo  and Jonathan P. K. Doye  and Sebastian E. Ahnert  and Ard A. Louis },
title = {Symmetry and simplicity spontaneously emerge from the algorithmic nature of evolution},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {11},
pages = {e2113883119},
year = {2022},
doi = {10.1073/pnas.2113883119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2113883119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2113883119},
abstract = {Why does evolution favor symmetric structures when they only represent a minute subset of all possible forms? Just as monkeys randomly typing into a computer language will preferentially produce outputs that can be generated by shorter algorithms, so the coding theorem from algorithmic information theory predicts that random mutations, when decoded by the process of development, preferentially produce phenotypes with shorter algorithmic descriptions. Since symmetric structures need less information to encode, they are much more likely to appear as potential variation. Combined with an arrival-of-the-frequent mechanism, this algorithmic bias predicts a much higher prevalence of low-complexity (high-symmetry) phenotypes than follows from natural selection alone and also explains patterns observed in protein complexes, RNA secondary structures, and a gene regulatory network. Engineers routinely design systems to be modular and symmetric in order to increase robustness to perturbations and to facilitate alterations at a later date. Biological structures also frequently exhibit modularity and symmetry, but the origin of such trends is much less well understood. It can be tempting to assume—by analogy to engineering design—that symmetry and modularity arise from natural selection. However, evolution, unlike engineers, cannot plan ahead, and so these traits must also afford some immediate selective advantage which is hard to reconcile with the breadth of systems where symmetry is observed. Here we introduce an alternative nonadaptive hypothesis based on an algorithmic picture of evolution. It suggests that symmetric structures preferentially arise not just due to natural selection but also because they require less specific information to encode and are therefore much more likely to appear as phenotypic variation through random mutations. Arguments from algorithmic information theory can formalize this intuition, leading to the prediction that many genotype–phenotype maps are exponentially biased toward phenotypes with low descriptional complexity. A preference for symmetry is a special case of this bias toward compressible descriptions. We test these predictions with extensive biological data, showing that protein complexes, RNA secondary structures, and a model gene regulatory network all exhibit the expected exponential bias toward simpler (and more symmetric) phenotypes. Lower descriptional complexity also correlates with higher mutational robustness, which may aid the evolution of complex modular assemblies of multiple components.}}


@article {Anishchenko2020.07.22.211482,
	author = {Anishchenko, Ivan and Chidyausiku, Tamuka M. and Ovchinnikov, Sergey and Pellock, Samuel J. and Baker, David},
	title = {De novo protein design by deep network hallucination},
	elocation-id = {2020.07.22.211482},
	year = {2020},
	doi = {10.1101/2020.07.22.211482},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {There has been considerable recent progress in protein structure prediction using deep neural networks to infer distance constraints from amino acid residue co-evolution1{\textendash}3. We investigated whether the information captured by such networks is sufficiently rich to generate new folded proteins with sequences unrelated to those of the naturally occuring proteins used in training the models. We generated random amino acid sequences, and input them into the trRosetta structure prediction network to predict starting distance maps, which as expected are quite featureless. We then carried out Monte Carlo sampling in amino acid sequence space, optimizing the contrast (KL-divergence) between the distance distributions predicted by the network and the background distribution. Optimization from different random starting points resulted in a wide range of proteins with diverse sequences and all alpha, all beta sheet, and mixed alpha-beta structures. We obtained synthetic genes encoding 129 of these network hallucinated sequences, expressed and purified the proteins in E coli, and found that 27 folded to monomeric stable structures with circular dichroism spectra consistent with the hallucinated structures. Thus deep networks trained to predict native protein structures from their sequences can be inverted to design new proteins, and such networks and methods should contribute, alongside traditional physically based models, to the de novo design of proteins with new functions.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/07/23/2020.07.22.211482},
	eprint = {https://www.biorxiv.org/content/early/2020/07/23/2020.07.22.211482.full.pdf},
	journal = {bioRxiv}
}

@inproceedings{NEURIPS2019_f3a4ff48,
 author = {Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Models for Graph-Based Protein Design},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{WU202118,
title = {Protein sequence design with deep generative models},
journal = {Current Opinion in Chemical Biology},
volume = {65},
pages = {18-27},
year = {2021},
note = {Mechanistic Biology * Machine Learning in Chemical Biology},
issn = {1367-5931},
doi = {https://doi.org/10.1016/j.cbpa.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S136759312100051X},
author = {Zachary Wu and Kadina E. Johnston and Frances H. Arnold and Kevin K. Yang},
keywords = {Deep learning, Generative models, Protein engineering},
abstract = {Protein engineering seeks to identify protein sequences with optimized properties. When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process. In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.}
}

@misc{fu2023latentdiffusionmodelprotein,
      title={A Latent Diffusion Model for Protein Structure Generation}, 
      author={Cong Fu and Keqiang Yan and Limei Wang and Wing Yee Au and Michael McThrow and Tao Komikado and Koji Maruhashi and Kanji Uchino and Xiaoning Qian and Shuiwang Ji},
      year={2023},
      eprint={2305.04120},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM},
      url={https://arxiv.org/abs/2305.04120}, 
}

@article {Zhang2023.12.13.571602,
	author = {Zhang, Yuyang and Ma, Zinnia and Gong, Haipeng},
	title = {TopoDiff: Improving Protein Backbone Generation with Topology-aware Latent Encoding},
	elocation-id = {2023.12.13.571602},
	year = {2023},
	doi = {10.1101/2023.12.13.571602},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The de novo design of protein structures is an intriguing research topic in the field of protein engineering. Recent breakthroughs in diffusion-based generative models have demonstrated substantial promise in tackling this task, notably in the generation of diverse and realistic protein structures. While existing models predominantly focus on unconditional generation or fine-grained conditioning at the residue level, the holistic, top-down approaches to control the overall topological arrangements are still insufficiently explored. In response, we introduce TopoDiff, a diffusion-based framework augmented by a global-structure encoding module, which is capable of unsupervisedly learning a compact latent representation of natural protein topologies with interpretable characteristics and simultaneously harnessing this learned information for controllable protein structure generation. We also propose a novel metric specifically designed to assess the coverage of sampled proteins with respect to the natural protein space. In comparative analyses with existing models, our generative model not only demonstrates comparable performance on established metrics but also exhibits better coverage across the recognized topology landscape. In summary, TopoDiff emerges as a novel solution towards enhancing the controllability and comprehensiveness of de novo protein structure generation, presenting new possibilities for innovative applications in protein engineering and beyond.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2023/12/14/2023.12.13.571602},
	eprint = {https://www.biorxiv.org/content/early/2023/12/14/2023.12.13.571602.full.pdf},
	journal = {bioRxiv}
}

@ARTICLE{Watson2023-eb,
  title     = "De novo design of protein structure and function with
               {RFdiffusion}",
  author    = "Watson, Joseph L and Juergens, David and Bennett, Nathaniel R
               and Trippe, Brian L and Yim, Jason and Eisenach, Helen E and
               Ahern, Woody and Borst, Andrew J and Ragotte, Robert J and
               Milles, Lukas F and Wicky, Basile I M and Hanikel, Nikita and
               Pellock, Samuel J and Courbet, Alexis and Sheffler, William and
               Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and
               Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli,
               Valentin and Mathieu, Emile and Ovchinnikov, Sergey and
               Barzilay, Regina and Jaakkola, Tommi S and DiMaio, Frank and
               Baek, Minkyung and Baker, David",
  abstract  = "There has been considerable recent progress in designing new
               proteins using deep-learning methods1-9. Despite this progress,
               a general deep-learning framework for protein design that
               enables solution of a wide range of design challenges, including
               de novo binder design and design of higher-order symmetric
               architectures, has yet to be described. Diffusion models10,11
               have had considerable success in image and language generative
               modelling but limited success when applied to protein modelling,
               probably due to the complexity of protein backbone geometry and
               sequence-structure relationships. Here we show that by
               fine-tuning the RoseTTAFold structure prediction network on
               protein structure denoising tasks, we obtain a generative model
               of protein backbones that achieves outstanding performance on
               unconditional and topology-constrained protein monomer design,
               protein binder design, symmetric oligomer design, enzyme active
               site scaffolding and symmetric motif scaffolding for therapeutic
               and metal-binding protein design. We demonstrate the power and
               generality of the method, called RoseTTAFold diffusion
               (RFdiffusion), by experimentally characterizing the structures
               and functions of hundreds of designed symmetric assemblies,
               metal-binding proteins and protein binders. The accuracy of
               RFdiffusion is confirmed by the cryogenic electron microscopy
               structure of a designed binder in complex with influenza
               haemagglutinin that is nearly identical to the design model. In
               a manner analogous to networks that produce images from
               user-specified inputs, RFdiffusion enables the design of diverse
               functional proteins from simple molecular specifications.",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  620,
  number    =  7976,
  pages     = "1089--1100",
  month     =  aug,
  year      =  2023,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}
