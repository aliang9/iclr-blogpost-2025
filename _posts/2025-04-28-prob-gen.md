---
layout: distill
title: Rethinking Probabilistic Protein Generation
description: Current protein generation models, while powerful, are constrained by evolutionary biases and struggle to explore beyond naturally occurring protein sequences. We propose a next-generation diffusion framework that incorporates bias-aware sampling and structural guidance to expand protein design beyond the evolutionary morphospace while maintaining structural validity. Our preliminary results demonstrate that this approach can triple the evolutionary distance from known proteins while preserving comparable structural metrics, suggesting a promising path toward discovering novel, functionally viable proteins beyond nature's repertoire.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Arthur Liang
#     url: "https://artliang.dev/"
#     affiliations:
#       name: MIT

# must be the exact same name as your blogpost
bibliography: 2025-04-28-prob-gen.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Probabilistic Generative Models for Protein Generation
  - name: Balancing Exploration and Exploitation with Diffusion Models
  - name: Beyond Diffusion and Next-Generation Approaches
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Probabilistic Generative Models for Protein Generation

Proteins are the molecular engines of life, orchestrating countless biological processes from catalyzing reactions to maintaining cellular structure. The challenge of designing new proteins for specific applications—whether novel enzymes or therapeutic molecules—requires navigating an astronomical sequence space. Even a modest protein of 100 amino acids presents $$ 20^{100} $$ possible sequences <d-cite key="nihNatureProtein"></d-cite>, yet natural evolution has explored only a tiny fraction of this space, constrained by fitness landscapes and biophysical rules <d-cite key="nihDarwinianEvolution"></d-cite>.

Probabilistic generative models have emerged as powerful tools for protein design <d-cite key="natureHighlyAccurate"></d-cite><d-cite key="nihEvolutionbasedModel"></d-cite>, learning from natural proteins to capture the statistical patterns underlying sequence-function relationships. Additionally, protein generation is a design problem that requires optimizing a sequence-to-function mapping: a high-dimensional, non-linear problem.

As such, the appeal of probabilistic generation lies in its dual ability to balance:
* **Exploration**: Generating diverse candidate sequences.
* **Exploitation**: Incorporating constraints to ensure biological relevance.

Probabilistic generation aligns well with the inherent structure and constraints of proteins, making it an ideal approach for their design:
1. It mirrors the dual requirements of stability and adaptability in proteins
2. It provides data-efficient learning that inherently incorporates uncertainty <d-cite key="natureUnifiedRational"></d-cite>
3. It enables high-throughput exploration through constrained random walks through the sequence space <d-cite key="riesselman2017deepgenerativemodelsgenetic"></d-cite>

Exploring and exploiting different models within this framework is important because they have a lot of downstream potential in fields like drug discovery, synthetic biology, and enzyme design as well as informing model architecture for tasks in other domains.

## Balancing Exploration and Exploitation with Diffusion Models

The success of diffusion models in particular stems from their ability to mimic evolutionary optimization <d-cite key="NEURIPS2020_4c5bcfec"></d-cite>.

Diffusion models for protein generation consist of a **forward process** (adding noise) and a **reverse process** (denoising). Noise is added to data $x_0$ over $T$ timesteps:

$$
q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) \mathbf{I})
$$

where $$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$$ controls the noise schedule. Then, the reverse process predicts the original data by removing noise step-by-step:

$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

The reverse diffusion process parallels how evolution refines random variations into functional sequences through iterative selection. This can be viewed as a Bayesian approach where the prior is a fitness landscape derived from evolutionary data and the posterior the viable sequences conditioned on this prior. 

Gradual refinement in the form of diffusion aligns with how proteins evolve through mutations and selection pressures and affords the computational flexibility to encode constraints (eg. amino acid frequency, structural motifs) for guided generation.

There are of course still a number of limitations including inaccuracies in modeling long range residue dependencies and how it isn't immediately clear that sequence generation by itself links to 3D structural constraints. Additionally, many models sample from a continuous latent space that projects back to discrete amino acids.

To tackle these concerns, latent diffusion models (LDM) and structure diffusion models (SDM) have been introduced. 
* For example, LatentDiff, by compressing representations into a low-dimension latent space, reduces the modeling space and relies on the hierarchical nature of proteins for faster generation. 
* RFDiffusion is able to control over specific structural topolgies and allow better capturing of geometric constraints of protein folds. This is analogous to learning the spontaneous folding process of proteins from non-functional/denatured states to one that is biologically plausible and functional while respecting physical constraints the whole time. 
* Similarly, TopoDiff incorporates an additional encoder module designed to learn the fixed-size latent topology representation from the training data similar to the VAE architecture. This allows for a more abstract view of generation at the topological level beyond just sequence with a structure-centric latent space optimized for designing at the domain level.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/latentdiff.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Pipeline of LatentDiff by Fu et al. depicting their latent diffusion framework where protein structures are encoded into latent representations via the encoder that are then perturbed into noise. Then during generation, using the learned denoising network, protein representations in the latent space are regenerated before decoding. <d-cite key="fu2023latentdiffusionmodelprotein"></d-cite>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/rfdiff.png" class="img-fluid" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/topodiff.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Examples of well-known structural motifs that arise during RFDiffusion generation by Watson et al. <d-cite key="Watson2023-eb"></d-cite>; t-SNE visualization of the latent space in TopoDiff by Zhang et al. demonstrating primitive substructure interpretability in LDM after including a global-structure encoding module <d-cite key="Zhang2023.12.13.571602"></d-cite>
</div>

However, current approaches face a more fundamental limitation: they are bound by what we call the "evolutionary morphospace"—the limited subset of sequence space that natural evolution through billions of years of selection has explored. This constraint manifests in two critical ways:
1. Dataset Bias: Training on evolutionary datasets inevitably encodes natural proteins' biases <d-cite key="rao2019evaluatingproteintransferlearning"></d-cite>
2. Limited Exploration: Models struggle to generate sequences that deviate significantly from known evolutionary patterns <d-cite key="elnaggar2021prottranscrackinglanguagelifes"></d-cite> <d-cite key="louis"></d-cite>

As such, current probabilistic methods, including diffusion, excel in sampling from nature’s limited morphospace but fail at generalizing or escaping its biases.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/enhanced-morphospace.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Visualization of the distinction between evolutionary accessible space and the total sequence space. Current diffusion models tend to sample near known functional niches where natural proteins cluster.
</div>

## Beyond Diffusion and Next-Generation Approaches

To transcend these limitations, there are several key directions we can pursue. One direction that has been popular in recent years following the development of multimodal foundation models is jointly modeling sequence and structure so as to incorporate physics and energy-based constraints alongside evolutionary patterns <d-cite key="NEURIPS2019_f3a4ff48"></d-cite>. Additionally, dynamic landscape modeling using reinforcement learning to simulate evolving fitness landscapes positions itself as a promising method for implementing active learning and incorporate experimental feedback in design <d-cite key="WU202118"></d-cite>.

But chiefly and of interset in this blog post, the most direct method is synthetic morphospace expansion but augmenting datasets with theoretical protein designs <d-cite key="Anishchenko2020.07.22.211482"></d-cite> and implementing bias-aware training that penalizes over-representation of common motifs. Towards the latter goal, we formulate a next-generation diffusion process as:

$$
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t \cdot \epsilon + \alpha(s, d) \cdot \nabla E(x_t)
$$

where:
* $$\mu_\theta(x_t, t)$$ is the standard diffusion mean prediction
* $$\sigma_t$$ is the diffusion variance schedule
* $$\epsilon$$ is random noise
* $$\alpha(s, d)$$ is an adaptive weight based on: $$s$$: structural validity score (TM-score) and $$d$$: evolutionary distance
* $$\nabla E(x_t)$$ is the exploration gradient

The adaptive weight $$\alpha$$ is computed as:

$$
\alpha(s, d) = \lambda \cdot \text{sigmoid}(\beta_s \cdot s - \beta_d \cdot d)
$$

where:
* $$\lambda$$ is the maximum exploration rate
* $$\beta_s, \beta_d$$ are sensitivity parameters for structure and distance

```python
def next_gen_diffusion_step(x_t, t):
    # Standard diffusion prediction
    eps_θ = model(x_t, t)
    
    # Compute structural validity and evolutionary distance
    struct_score = compute_tm_score(x_t)
    evo_dist = compute_evolutionary_distance(x_t)
    
    # Balance exploration vs validity
    α = adaptive_weight(struct_score, evo_dist)
    exploration_term = gradient_away_from_known_proteins(x_t)
    
    # Modified update combining diffusion and exploration
    x_t_prev = diffusion_update(x_t, eps_θ, t) + α * exploration_term
    
    return project_to_valid_structure(x_t_prev)
```

<div class="caption">
    Pseudocode for Next-Gen Diffusion Step
</div>

As a proof of concept, we conducted a comparative analysis of protein sequence generation using 100 samples each from standard diffusion and our next-generation approach. 

The standard diffusion model, trained on natural protein sequences from PDB, generated samples with high structural validity (TM-score: 0.85, pLDDT: 90.0) but remained conservative in exploring sequence space, with samples averaging only 0.25 evolutionary distance from known proteins. In contrast, our next-generation approach, which incorporates bias-aware sampling and structural guidance, achieved comparable structural metrics (TM-score: 0.82, pLDDT: 87.0) while more than tripling the evolutionary distance (0.75) from known sequences. 

This increased exploration also resulted in the discovery of 28 unique structural motifs compared to 12 in the standard approach. Notably, our method maintained structural viability even at higher evolutionary distances by employing a dynamic weighting scheme (α) that balances exploration with structural constraints. The distribution of evolutionary distances indicates how our approach shifts the sampling distribution toward more novel sequences while maintaining the total probability mass, indicating efficient exploration of previously unexplored protein space.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/metric-comparison-natural.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    In a simple experiment swapping in the proposed next-gen diffusion step, we achieve broader exploration and samples that are more distant from known protein families. However, we observe a trade-off between structural validity and novelty.
</div>

## Conclusion

The protein sequence space is vast, yet evolution has explored only a constrained morphospace due to functional and stability constraints. Current probabilistic models, trained on evolutionary datasets, inherit these biases, creating a paradox: they excel at generating proteins similar to known ones but struggle to venture beyond evolution's footprints. While this conservatism ensures stability, it also limits our ability to discover truly novel proteins that could revolutionize medicine, materials science, and biotechnology.

Our preliminary experiments suggest a promising path forward. By modeling and then systematically relaxing evolutionary constraints while maintaining physical viability, we can begin to explore the "dark matter" of protein space. Just as cartographers of old combined known landmarks with theoretical predictions to chart unexplored territories, our next-generation models must balance evolutionary wisdom with computational exploration. We propose that next-generation models should aim to explicitly disentangle evolutionary biases while enabling exploration of sparsely populated or hypothetical regions of protein design space. 

This vision requires several key advances:
* Adaptive sampling strategies that dynamically balance structural stability with novelty
* Physics-informed constraints that replace evolutionary biases
* Multi-scale validation combining in silico prediction with targeted experimental feedback

The future of protein design lies not in simply mimicking evolution, but in understanding and then transcending its limitations. By developing models that can venture beyond the evolutionary morphospace while maintaining physical realizability, we open the door to an entirely new universe of proteins with properties and functions never seen in nature.

